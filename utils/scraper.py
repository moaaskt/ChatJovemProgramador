import requests
from bs4 import BeautifulSoup
import json
import re


def raspar_sobre():
    try:
        url = "https://www.jovemprogramador.com.br/sobre.php"
        response = requests.get(url)
        soup = BeautifulSoup(response.text, "html.parser")

        secao_sobre = soup.find("div", class_="fh5co-heading")

        if not secao_sobre:
            return {"sobre": "Informa√ß√µes n√£o dispon√≠veis."}

        textos = []
        for p in secao_sobre.find_all("p"):
            texto = p.get_text(strip=True)
            if texto and len(texto) > 20:
                textos.append(texto)

        textos_unicos = []
        visto = set()
        for texto in textos:
            if texto not in visto:
                visto.add(texto)
                textos_unicos.append(texto)

        return {"sobre": "\n\n".join(textos_unicos[:5])}

    except Exception as e:
        print(f"Erro ao raspar 'sobre': {e}")
        return {"sobre": "Erro ao carregar dados."}


def raspar_duvidas():
    try:
        url = "https://www.jovemprogramador.com.br/duvidas.php"
        response = requests.get(url)
        soup = BeautifulSoup(response.text, "html.parser")
        duvidas = {}

        accordion = soup.find("div", class_="accordion")
        if not accordion:
            return {"duvidas": {}}

        itens_duvida = accordion.find_all("div", recursive=False)

        for item in itens_duvida:
            pergunta = item.find("h4").get_text(strip=True) if item.find("h4") else ""
            resposta_div = item.find("div", class_="collapse")
            resposta = (
                resposta_div.find("p").get_text(strip=True)
                if resposta_div and resposta_div.find("p")
                else ""
            )

            if pergunta and resposta:
                duvidas[pergunta.strip()] = resposta.strip()

        return {"duvidas": duvidas}
    except Exception as e:
        print(f"Erro ao raspar 'duvidas': {e}")
        return {"duvidas": {}}


def raspar_cidades():
    try:
        url = "https://www.jovemprogramador.com.br/sobre.php"
        response = requests.get(url)
        soup = BeautifulSoup(response.text, "html.parser")

        # Encontra o par√°grafo que come√ßa com "Para a edi√ß√£o de"
        for p in soup.find_all("p"):
            if p.get_text(strip=True).startswith("Para a edi√ß√£o de"):
                # O pr√≥ximo elemento <p> cont√©m as cidades
                cidades_paragraph = p.find_next_sibling("p")
                if cidades_paragraph:
                    # Extrai o texto dentro da tag <strong>
                    cidades = cidades_paragraph.find("strong").get_text(strip=True)
                    return {"cidades": cidades}

        return {"cidades": "Lista de cidades n√£o encontrada na p√°gina."}

    except Exception as e:
        print(f"Erro ao raspar cidades: {e}")
        return {"cidades": "Erro ao carregar lista de cidades."}


def raspar_noticias():
    """Raspa TODAS as not√≠cias da p√°gina usando a estrutura HTML correta."""
    print("üì∞ Raspando TODAS as not√≠cias com o novo seletor...")
    try:
        url = "https://www.jovemprogramador.com.br/noticias.php"
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            print(f"‚ùå ERRO: Falha ao acessar a p√°gina. C√≥digo: {response.status_code}")
            return {"noticias": "Erro de conex√£o com o site."}

        soup = BeautifulSoup(response.text, "html.parser")
        noticias = []

        # NOVA ESTRAT√âGIA: Encontrar os cont√™ineres das colunas, que s√£o mais confi√°veis.
        # A classe 'col-md-4' parece ser o cont√™iner de cada not√≠cia.
        cards_containers = soup.find_all("div", class_="col-md-4")
        print(f"Encontrados {len(cards_containers)} poss√≠veis cont√™ineres de not√≠cia.")

        for container in cards_containers:
            # Dentro de cada cont√™iner, procuramos os elementos espec√≠ficos da not√≠cia
            titulo_tag = container.find("h3", class_="title")
            resumo_tag = container.find("p")
            link_tag = container.find(
                "a"
            )  # O link geralmente envolve a imagem ou o card todo

            # S√≥ consideramos um card v√°lido se ele tiver um t√≠tulo e um link
            if titulo_tag and link_tag and "href" in link_tag.attrs:
                titulo = titulo_tag.get_text(strip=True)
                link_relativo = link_tag["href"]
                link = f"https://www.jovemprogramador.com.br/{link_relativo}"

                # Pega o resumo, se existir, sen√£o define um padr√£o.
                resumo = (
                    resumo_tag.get_text(strip=True)
                    if resumo_tag
                    else "Resumo n√£o dispon√≠vel."
                )

                noticias.append({"titulo": titulo, "link": link, "resumo": resumo})

        if noticias:
            print(f"‚úÖ SUCESSO! Total de {len(noticias)} not√≠cias extra√≠das.")
        else:
            print(
                "‚ö†Ô∏è AVISO: Nenhuma not√≠cia foi extra√≠da. A estrutura pode ter mudado novamente."
            )

        return {"noticias": noticias}

    except Exception as e:
        print(f"‚ùå ERRO INESPERADO: {e}")
        return {"noticias": "Erro durante a execu√ß√£o do scraper."}


def raspar_ser_professor():
    """Raspa as informa√ß√µes da p√°gina 'Quero Ser Professor'."""
    print("üßë‚Äçüè´ Raspando informa√ß√µes sobre 'Quero Ser Professor'...")
    try:
        url = "https://www.jovemprogramador.com.br/queroserprofessor/"
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            print(
                f"‚ùå ERRO ao acessar a p√°gina 'Quero Ser Professor'. C√≥digo: {response.status_code}"
            )
            return {"ser_professor": {}}

        soup = BeautifulSoup(response.text, "html.parser")

        # Encontrar a informa√ß√£o sobre vagas abertas
        h3_vagas = soup.find("h3", string=re.compile(r"Acesse o portal do Senac SC"))
        link_vagas_abertas = ""
        if h3_vagas:
            link_tag = h3_vagas.find_next("a", class_="btn-primary")
            if link_tag and "href" in link_tag.attrs:
                link_vagas_abertas = link_tag["href"]

        # Encontrar a informa√ß√£o sobre registrar interesse
        h3_interesse = soup.find("h3", string=re.compile(r"N√£o tem vaga dispon√≠vel"))

        # Montar o dicion√°rio de dados se as informa√ß√µes foram encontradas
        if h3_vagas and h3_interesse and link_vagas_abertas:
            dados_professor = {
                "titulo": "Como se tornar um professor do Jovem Programador",
                "vagas_abertas": {
                    "texto": "Para conferir as vagas de professor que est√£o abertas, o candidato deve acessar o portal de talentos do Senac SC.",
                    "link": link_vagas_abertas,
                },
                "registrar_interesse": {
                    "texto": "Caso n√£o encontre uma vaga para a sua cidade no portal do Senac, o candidato pode registrar seu interesse preenchendo o formul√°rio na p√°gina 'Quero Ser Professor' no site do Jovem Programador.",
                    "link_pagina": url,
                },
            }
            print("‚úÖ Informa√ß√µes de 'Quero Ser Professor' extra√≠das com sucesso.")
            return {"ser_professor": dados_professor}
        else:
            print(
                "‚ö†Ô∏è AVISO: N√£o foi poss√≠vel extrair as informa√ß√µes da p√°gina 'Quero Ser Professor'."
            )
            return {"ser_professor": {}}

    except Exception as e:
        print(f"‚ùå ERRO INESPERADO ao raspar 'Quero Ser Professor': {e}")
        return {"ser_professor": {}}


def salvar_dados():
    print("\nüöÄ Iniciando raspagem completa do site...")
    dados = {
        "sobre": raspar_sobre()["sobre"],
        "duvidas": raspar_duvidas()["duvidas"],
        "cidades": raspar_cidades()["cidades"],
        "noticias": raspar_noticias()["noticias"],
        "ser_professor": raspar_ser_professor()["ser_professor"],  # <-- NOVA LINHA
    }

    with open("dados.json", "w", encoding="utf-8") as f:
        json.dump(dados, f, ensure_ascii=False, indent=2)
    print("\n‚úÖ Dados atualizados e salvos com sucesso em 'dados.json'")


if __name__ == "__main__":
    salvar_dados()
